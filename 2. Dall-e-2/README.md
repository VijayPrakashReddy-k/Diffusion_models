## ğŸ“Œ Hierarchical Text-Conditional Image Generation with CLIP Latents (Dall-e-2)

*There are the four key high-level concepts:*

    â€¢ CLIP: Model that takes image-caption pairs and creates â€œmentalâ€ representations in the form of vectors, called text/image embeddings.

    â€¢ Prior model: Takes a caption/CLIP text embedding and generates CLIP image embeddings.

    â€¢ Decoder Diffusion model (unCLIP): Takes a CLIP image embedding and generates images.

    â€¢ DALLÂ·E 2: Combination of prior + diffusion decoder (unCLIP) models.
